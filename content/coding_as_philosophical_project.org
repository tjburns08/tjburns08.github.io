#+Title: If Plato lived today, he'd be coding
#+Author: Tyler Burns
#+Date: January 15, 2023 - January 21, 2024

[[./index.html][Home]]

-----
/The knowledge of which geometry aims is the knowledge of the eternal./\\

Plato, /[[https://www.gutenberg.org/files/1497/1497-h/1497-h.htm][The Republic]]/, Book VII, 52.
-----
/The purpose of computing is insight, not numbers./\\

Richard Hamming, /[[https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=numerical.methods.for.scientists.and.engineers_2ed_hamming_0486652416.pdf][Numerical Methods for Scientists and Engineers]]/
-----

* From geometry to computer science
Plato valued geometry highly. But he was not a geometer. He did not use geometry for anything practical in his life. He saw it as a stepping stone to the study of philosophy. In
fact, on the door to his Academy, there was a sign that said "let no one ignorant of geometry enter."

In my life, I have found computer science to be the modern heir to geometry in terms of a stepping stone to living the philosophical life. Why does this matter? Because it could very well be that in the next few years, generative AI renders human-based coding obsolete at the practical level. If this is the case, I will continue coding and teaching people how to code. Why? Because through computer science I can truly engage in the philosophical life the way Plato did using geometry. Computer science taught me how to think. 

* The scientific method
We all learned the scientific method in school.\\

1. Ask a question\\
2. Do some research as to what is known already\\
3. Form a hypothesis\\
4. Run an experiment to test the hypothesis\\
5. Analyze the data\\
6. Form a conclusion\\
7. For each follow up question, go to step 1.\\
   
The conclusion leads to follow-up questions and you're at the top of the loop again. Importantly, in my experience, my hypothesis is partially or fully wrong quite often. Each time I'm wrong, my intuition around how the work works improves. Furthermore, I get a chance to do a mental stack trace to see if I can locate the fallacies in my thinking that led to the wrong hypothesis. Sometimes, it's simply that biology is complicated. Often, I didn't read up enough on Gene X, or some sort of cognitive bias (wishful thinking, because I want to get the high-end publication) clouds my judgment. In this regard, through repeated experiments, where I put my hypothesis on the line, I become a better thinker. 

Now, in my little corner of biology, step 4 and step 5 would both take anywhere from days to months. If you're developing a mouse model from scratch, one experiment could take years. If one of your goals is to become better at thinking, then this can really slow you down.

When I started pursuing computer science halfway through graduate school, I was surprised at how much this process sped up. When I was de-bugging code, I would run experiments at the rate of 1 or more per minute. Often for several hours. Again, each experiment where I was wrong, each error message, gave me the opportunity to become a better thinker. I learned early on to love the error message. You spend a huge chunk of time, upwards of half of your time, de-bugging code.

There is a level above that of the rapid-fire experiments improving your ability to think via the scientific method. The higher level is the fact that you're not trying to understand billions of years of evolution. You're trying to understand code that you wrote. You converted your logical thoughts into computer language, and then the computer gave you an error message. Your mental model about how this was going to work was wrong. So when you find the bug, you have located an error in your thinking. Then you learn from it. Then you become a better thinker.

* Platonic forms
Plato believed that for all things, there was an ideal platonic form of the thing. The example used when I was learning this was a chair. Each chair was imbued with some amount of chair-ness. In theory, there would be a chair out there that would have the maximum amount of chairness. It would be as much of a chair as anything could be. It would be the platonic form of a chair.

Coding, and computation in general, gives me a similar feeling as staring at a Platonic form. Specifically, there is something very special about cellular automata. This is where you take a simple set of rules and run them, which produces emergent complexity. Some cellular automata are more elaborate than others despite having a rule set of similar complexity. Want an example: how about [[https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life][Conway's Game of Life]]. Take a grid. Each square on the grid can either be live or dead. For each square in the grid, follow these rules:

1. Any live cell with fewer than two neighbors dies.
2. Any live cell with two or three neighbors lives.
3. Any live cell with more than three neighbors dies.
4. Any dead cell with three neighbors becomes a live cell.

That's it. Run it and you can get [[https://www.youtube.com/watch?v=cTZkEAYeRis][incredible patterns]]. If you're curious, have a look here in the Life Wiki at the [[https://conwaylife.com/wiki/Category:Patterns][various patterns]] that have been found. Running Conway's Game of Life is the computational equivalent of looking at a drop of pondwater under the microscope. Included in these patterns is a [[https://en.wikipedia.org/wiki/Turing_machine][Turing machine]], a common model for a computer used in theoretical computer science. The picture of a Turing machine below, implemented in Conway's Game of Life, is from the respective page in [[https://conwaylife.com/wiki/Turing_machine][Life Wiki]].

[[file:images/2024-01-20_22-17-35_Screenshot 2024-01-20 at 22.17.16.png]]

In other words, Conway's game of life is [[https://en.wikipedia.org/wiki/Turing_completeness][Turing complete]], meaning that any form of computation that exists, from Tetris to ChatGPT, is theoretically implementable using only Conway's game of life patterns (inefficient, but possible).

Ok, so what does this have to do with Platonic forms? Well for me, when I first came across Conway's Game of Life when I was 16, there was a sort of universe-ness that was totally maxed out. It was the first time where I could conceive of our universe being made up of something like this at the very bottom. Even if it wasn't, I got the idea in my head that [[https://en.wikipedia.org/wiki/Emergence][emergent complexity]] (which is perhaps the -ness that is being maxed out here) could give rise to way more than I had ever thought.

So it was perhaps with Conway's game of life that I found the Platonic form of emergence, or perhaps what Stuart Kauffman might call [[https://www.amazon.de/-/en/Stuart-Kauffman/dp/0195079515][the Origins of Order]]. It wasn't until I was 28 and was learning how to code that I had this feeling again, and I knew I was going to pursue it for as long as I possibly could.

* An expanded language
** Levels of analysis
My first computer science class was in Java. My second one was in C++. These are lower-level langugaes as compred to R and Python, the two languages that I use these days. It was through programming that I really solidified the concept of levels of analysis. We all have a general idea of what it is, like the xkcd comic [[https://xkcd.com/435/][here]]. That pschology is just applied biology is just applied chemistry is just applied physics, etc. I'll add that that by saying that as a biologist, the best biologists I know are actually chemists in disguise.

In terms of computer science, we have what are broadly called low-level languages and high-level languages. These terms are relative to your programming language of choice. If we look at the printing of "hello world" in Python for example, it looks like this:

#+begin_src python
print("hello world")
#+end_src

That's it. A single command. Then you run it. Then you get "hello world" on the console. But there's a ton of stuff that happens under the surface. To give you an idea of what that looks like, let's go with a lower level language. C.

#+begin_src c
#include <stdio.h>

int main() {
    printf("hello world");
    return 0;
}
#+end_src

So here, we need to include a library that allows us to do input/output things, which gives us the function printf. It's not just built into the language. We have to end each line with a semicolon. We have int main() which is our main function that must be called to run the thing. We're declaring the type of thing that the function returns. In this case an integer.

This brings us to the point that in C (and many other languages) you have to declare the type of object you're using. So if you have a variable x you want to set to 5, you have to say int x = 5, whereas in python you'd say x = 5. And you need a statement that the function returns. In this case 0, which by convention terminates the program. So you're also telling the computer when it terminates. It doesn't just figure it out. So there's a lot more you have to keep track of. And if you're just trying to analyze some data, it's way more convenient for the computer to sweep it under the rug.

There's a whole other piece here that I'm not going to talk about for the sake of brevity: while R and python are interactive, where you can simply type things in and they run automatically, C and other lower level languages are entirely compiled. Rather than programming interactively, you have to [[https://en.wikipedia.org/wiki/Compiler][compile]] it first, or convert it into the binary machine code that will be understood by the computer's hardware. This requires the use of a compiler to turn your C file into an executable binary file, which is then read by the computer, which only then produces "hello world."

But this is just the top of the rabbit hole. If you /really/ want to know what's going on, let's look at an even lower level language: Assembly. This is the language underneath C and everything else (save machine code). If you code in python, then C is a lower level language. If you code in assembly (which is very rare these days), then C is a higher level language. So I'm going to give you the Assembly code for printing out hello world for the ARM64 chip, which my current computer runs. This is the first point: when you're coding in Assembly, you're dealing with a different language for each chip. Now, there's a lot going on below, so if you want a better explanation from someone who actually codes in assembly, please watch this [[https://www.youtube.com/watch?v=d0OXp0zqIo0][video by Chris Hay]], which gets credit for the code and the explanation below.

#+begin_src assembly
// hello world

.global _start
.align 2

// main
_start:
    b _printf
    b _terminate

_printf:
    mov X0, #1      // stdout
    adr X1, helloworld      // address of hello world string
    mov X2, #12     // length of hello world
    mov X16, #4      // write to stdout
    svc 0           // syscall

_terminate:
    mov X0, #0      // return 0
    mov X16, #1     // terminate
    svc 0           // syscall

// Hello world string
helloworld: .ascii "hello world\n"
#+end_src

Ok, so what is going on here? Now we're giving that computer direct, low-level commands to the processor. Let's focus on what's going on underneath my comment "//main." Without going into a larger discussion around computer architecture, we'll summarize the procedure. You are in no way supposed to fully get what's going on here. You're just supposed to understand that there's a lot that happens under the hood. With that in mind, read on.

We first have to prepare the computer to output "hello world." In the _printf function, we're going to set the output stream (stdout) in the register (CPU memory slot) X0. Then we're going to create a memory address for our string, which we're naming "helloworld" and store /[[https://en.wikipedia.org/wiki/Memory_address][the address]]/ (not the string, just the place in memory that will hold the string) in register X1. Then we're going to tell the computer the length of our string of interest (count the number of characters, including whitespace, plus the newline character), which is 12 characters, which is 12 [[https://en.wikipedia.org/wiki/Byte][bytes]], and store that in register X2. In X16, we're going to place the instruction to write to stdout. Then we call svc 0, which actually requests the operating system to execute _printf.

Then, we have to tell the computer to terminate the program, which is the _terminate function that we define. The equivalent of return 0 from C is moving the NULL command into register X0. This means that the program executed successfully. Then we move the exit command into X16, where we previously were holding the "write to stdout" command. Then we call svc 0 again, which requests the operating system execuite _terminate after displaying "hello world."

Then, like C, there's a song and dance that converts this instruction set into binary machine code that the computer can read, and then it can actually output "hello world." And then we're done.

So I'm going to cut and paste the python code from above to remind you the sheer volume of things that are swept under the rug:

#+begin_src python
print("hello world")
#+end_src

Ok, so how the heck does this relate to philosophy? Well, we started with a discussion of levels of analysis from psychology to physics. Then we moved to the equivalent in computer science. What you learn in computer science in real time is that understanding what's going on at least one level below what you're doing makes you a much better programmer.

How do I mean? If I run into a bug in python or R, the issue could very well be a lower level issue, the same way that treating disease has you working with chemistry to treat a problem in biology. Quite a lot of so-called hacking (both security hacking and innovation) works by means of understanding things one or more levels underneath what you're doing. A much larger discussion of this can be found from this amazing [[https://gwern.net/unseeing][article]] written by Gwern that I've read many times. But let me paste the punchline, as food for thought:

/In each case, the fundamental principle is that the hacker asks: “here I have a system W, which pretends to be made out of a few Xs; however, it is really made out of many Y, which form an entirely different system, Z; I will now proceed to ignore the X and understand how Z works, so I may use the Y to thereby change W however I like”./

In other words, the hacker looks at a thing, and realizes that the thing is merely an abstraction made out of atoms or bits or whatever other low-level object, and it's just a matter of moving those bits/atoms around in a particular way, and they get what they want. I'll paste another bit from Gwern's article to really solidify this.

/In hacking, a computer pretends to be made out of things like ‘buffers’ and ‘lists’ and ‘objects’ with rich meaningful semantics, but really, it’s just made out of bits which mean nothing and only accidentally can be interpreted as things like ‘web browsers’ or ‘passwords’, and if you move some bits around and rewrite these other bits in a particular order and read one string of bits in a different way, now you have bypassed the password./

There is one more insight here that I have to continually remind myself over and over. This comes from the analogy to speedrunning, which is a hobby in video gaming where you try to beat a game as fast as possible. In speedrunning, you have to know both how to hack the game, and you have to be maximally skilled. You can't just do a hack and call it a day (everyone is looking for the "hack" these days). From Gwern:

/In speed running (particularly TASes), a video game pretends to be made out of things like ‘walls’ and ‘speed limits’ and ‘levels which must be completed in a particular order’, but it’s really again just made out of bits and memory locations, and messing with them in particular ways, such as deliberately overloading the RAM to cause memory allocation errors, can give you infinite ‘velocity’ or shift you into alternate coordinate systems in the true physics, allowing enormous movements in the supposed map, giving shortcuts to the ‘end’ of the game./

To get a feel for this, have a look at this history of [[https://www.youtube.com/watch?v=WNgJCe3HSGY][Mario Wonder speedrunning]] (which includes info about speed runs in other video games). Someone learns some exploit that the game designers did not anticipate, then everyone is doing that exploit with maximal skill with the character, and then someone learns a new exploit, and the cycle continues. So you have to know both the hacks (be able to operate at lower levels) /and/ have maximum talent (be able to operate at higher levels). Put differently, a biologist needs to know chemistry, but also needs to be a biologist.

Taken together, in terms of being a better thinker, it's good to know how things work at least one level under whatever you're doing. I'm not the first to say this by any means. Are you a biologist, at least be familiar with if not competent in chemistry. Are you a python programmer, at least be familiar with if not competent in C. Broadly learn how things work (which is really just another way of saying to look at a thing at a level of analysis below wherever you're at). Coding really solidifies this concept and teaches you what it feels like to think at a high level (program in python) versus to think at a low level (program in C or Assembly), and the value of both. Again, I primarily use R and python, but being familiar with the lower level languages, and the thinking habits they have taught me, has paid off many times over.

** Recursion
-----
/“I enjoy acronyms. Recursive Acronyms Crablike "RACRECIR" Especially Create Infinite Regress”/\\

― Douglas Hofstadter, /Gödel, Escher, Bach: An Eternal Golden Braid/
-----

Computer science gives us data structures and algorithms that don't come easy to standard spoken language. What is recursion? You're defining a function where the function is executed in the function definition. Ok, that's a mouthful. Let's try again. What is recursion?

#+begin_src python
def factorial(x):
  if x < 2:
    return 1
  else:
    return x * factorial(x - 1)
#+end_src

Still a bit mind-bending if you've never seen this before. If this is new to you, get out some paper and draw out the procedure for factorial(5), treating the above as a recipe. Recursion is much easier to explain, think through, and understand in code. There's a fantastic book called /Gödel, Escher, Bach/ by Douglas Hofstadter. It happens to be largely about recursion: these functions that talk about themselves. I've read it 3 times: once when I was 14, once when I was in my early 20s, and once when I was in my early 30s. It more or less went over my head the first two times, but I finally understood it the third time around. Why? Because knowing computer science, even at the rudimentary level, helped me understand what he was talking about.

** Graphs
Ok, how about a practical example for biologists. What is a cell signaling pathway? Well, to massively oversimply, you have messages being passed from protein to protein all the way down to the DNA where some sort of effector (eg. a transcription factor) does a thing to the DNA. What if you wanted to model that? How would you do it? Well, in computer science (and discrete math) there is a data structure called a graph that allows for one to wire up a pathway /in silico./ This is a graph as in a mathematical abstraction of a network, not to be confused with a biaxial plot.

Here's what the graph representation of a piece of a [[https://en.wikipedia.org/wiki/MAPK/ERK_pathway][pathway]] looks like in base python, using a dictionary (again, confusing wording...it's a look-up table):

#+begin_src python
graph = {
   'RAS':'RAF',
   'RAF':'MEK',
   'MEK':'MAPK',
   'MAPK':['MNK', 'RSK', 'MYC']
}
#+end_src

So now let's [[https://omnipathdb.org/][wire]] [[https://reactome.org/][one]] [[https://www.genome.jp/kegg/pathway.html][up]]. Ok, done. What do I get from that? Well, one very fundamental question in graph theory is what are the "central" regions of a graph? This is called [[https://en.wikipedia.org/wiki/Centrality][centrality]]. Degree centrality tells us how many friends each node has. Betweenness centrality tells us what regions in the network have the most shortest paths that run through them. Think of the Bay Bridge from Oakland to San Franscisco. Commuters know that, minus traffic, that is the quickest path to San Francisco for a lot of the East Bay and beyond. The Bay Bridge would have a high betweenness entrality. But with this metric you can quantify that and compare it to the San Mateo bridge to the south. Such is the same with signaling pathways. Assuming you have a good dataset, you can start interrogating these pathways in terms of regions that are relevant to whatever your intent is.

How do I know this? I spent three years doing just this for a client of mine. The use case is simple (though the implementation is complicated): can we find druggable regions of the network that will lead to the change that we want given the intent of the company? It would have been very hard, if not impossible, to do this kind of work without the intuition and use of a graph.

** Models and intelligence
[[https://www.youtube.com/watch?v=Lhl51bZQlM8][Socrates wants to know what virtue is]]. So he asks you "what is virtue?" In the stereotypical dialogues with Socrates, he asks you question after question until you contradict yourself, proving that you don't know what you're talking about nearly as well as you thought. Now, one interpretation of Socrates that I particularly like is that he did this for the purpose of inducing [[https://en.wikipedia.org/wiki/Aporia][aporia]]. This is a state where you're not really talking or thinking verbally anymore because you doubt all of your words. What does this do? Well, what is left when verbal thinking is gone? Nonverbal thinking. So in this interpretation of Socrates, he's trying to get you to realize that there is a lot in this world that cannot be explained precisely by words.

AI leader Joscha Bach (yes, related to Bach the musician) has an interesting angle to these big questions. He translates them into data structures and algorithms and then attempts to explain them through that lens. Let me give you a simple example of how he thinks so you can get a feel for what this sounds like:
\\
\\
-----
/“An organism is not a collection of cells; it’s a function that tells cells how to behave. And this function is not implemented as some kind of supernatural thing, like some morphogenetic field, it is an emergent result of the interactions of each cell with each other cell.”/\\
-----
\\
Now with his mindset in mind, let's move to the brain. In his [[https://www.youtube.com/watch?v=P-2P3MSZrBM][podcast with Lex Fridman]], when he's talking about the definition of intelligence, he says:
\\
\\
-----
/So intelligence, I think, is the ability to model. It's not necessarily goal directed rationality or something, many intelligent people are bad at this. But it's the ability to be presented with a number of patterns and see a structure in those patterns and be able to predict the next set of patterns, to make sense of things./
-----
\\
Ok, so now we have this idea of intelligence as making relevant models of the world. We'll get into a technical definition of models in a minute. But we started off with virtue, so let's move back there with this foundation. Joscha Bach is later talking about [[https://en.wikipedia.org/wiki/Thomas_Aquinas][Thomas Aquinas]], and he says:
\\
\\
-----
/And then he says that there are additional rational principles that humans can discover and everybody can discover them so there are universal. If you are sane you, should understand, you said to submit to them because you can rationally deduce them. And these principles are roughly: you should be willing to self-regulate correctly. You should be willing to do correct social regulation, inter-organismic. You should be willing to act on your models so you have skin in the game. And you should have goal rationality, you should be choosing the right goals to work on. And so basically these three rational principles, goal rationality he calls prudence or wisdom, social regulation is justice, the correct social one, and the internal regulation is temperance. And this thing, willingness to act on your models is courage./
-----

** Models lead to questions
Ok, so back to the original question about what is virtue? Here, we have virtue through the framework of making relevant models of the world, and importantly the willingness to act on them. If we can get to a computational definition of models and build up from that, maybe we can gain some further insights or at least come up with interesting questions and hypotheses we wouldn't have otherwise thought of.

Let's look at a simple model. If we go with housing prices, a simple model will take on the y = mx + b form. But there will be a number of mx's. We'll use w rather than m, to denote weights, where the x will be some characteristic that is weighted based on how well it predicts housing prices.

#+begin_src python
housing_prices = w1*num_rooms + w2*square_footage + w3*distance_from_beach + w4*school_district + b
#+end_src

If the school district mattered the most, then w4 would be really high. If the number of rooms didn't matter, then w1 would be really low. These models are trained on data, and that's how the weights are figured out that predict the price of the house. This is well outside of the scope of the article. But just know that a lot of AI models are complicated versions of the above, trained on lots (upwards of billions) of data points.

The last piece you need to understand the rest of this is that neural nets (the basis of a lot of modern AI) don't necessarily start with specific characteristics like number of rooms. They might just receive images of houses and the price of the house. They'll learn things over time, like size, number of floors, condition of the front yard, and things of that nature. So you have weights that are learned, but the characteristics that are being weighted are not known. Just w1*x1, w2*x2, w3*x3, etc. That's why they're often called black boxes. 

Now if you were programming an AI agent to understand and act virtuously, how would you do it? One way would be to build a set of [[https://www.youtube.com/watch?v=qv6UVOQ0F44][reinforcement learning models]] trained on real word data (images, videos, stories) that correspond to things like courage, temperance, justice, and wisdom. Another way that is currently en vogue is to assume the [[https://gwern.net/scaling-hypothesis][scaling hypothesis]] and throw as much data as you possibly can at a [[https://arxiv.org/abs/1706.03762][transformer]]. This gives us large language models that can potentially answer questions as if they were Socrates by virtue of grokking it from the sheer volume of data in its training set. At the time of writing [2024-01-20 Sat] we have yet to see whether or not this strategy will scale all the way to AGI.

So do our brains work like this? It's obviously [[./its_more_complicated_than_that.html][more complicated than that]], but at least this allows us to start asking actionable questions: if these are models, are they pre-trained? Let's look at toddlers reacting to just and unjust actions. Are these models trainable? Let's examine human cognitive and emotional development across cultures that have different value systems. Are they centralized (remember centrality from the section on graphs)? Let's do a neuroimaging study where we show subjects instances of courage, cowardice, justice, injustice, etc, and see what regions of the brain light up. Do different regions light up, or are they roughly the same for each virtue or vice? Socrates would get me to contradict myself nonetheless. I would concede defeat, but I would tell him that at least it's getting me to ask some good and testable questions. Socrates, who prototypically values asking questions, would probably understand.

What computational definitions and analogies do is cut into the space of things that are [[./fear_the_unword.html][hard to put into words]]. To say that something might be a reinforcement learning model in our brain is a more satisfying and actionable hypothesis than just telling Socrates "you know it when you see it."

* Conclusion
As I get older, I increasingly value the endless pursuit of wisdom. Cognitive scientist John Vervaeke likes to say that the child is to the adult as the adult is to the sage. I like that framing. Now at least for me, I use computer science along with the scientific method as a base for my thinking and [[./dialectic.html][sensemaking]]. We all know how to do the scientific method, but computer science is both a way to intensively put the scientific method into practice, and way to expand your lexicon to include things that are otherwise hard to put into words. As such, I see computer science as a solid foundation for modern philosophy, the way Plato saw geometry in his time.

The actionable advice I would give is to gain a basic understanding of computer science, even if AI automates the whole thing. It doesn't take very long to [[./learn_bioinformatics.html][learn how to think computationally]]. An intro course on python will teach you the basic data structures, algorithms and concepts that I still use today. Writing a couple of scripts that do things you care about will put the knowledge in practice, and you'll see what I mean about the intensive practice of the scientific method.

Now all of this being said, I have to concede that there is more to philosophy to thinking computationally. Iain McGilchrist argues that a lot of philosophy comes out of really contemplating on, meditating on, and cultivating a sense of awe and wonder and the sacred.  However, when it comes time to put those insights to paper, computer science is the modern way to train one to think rigorous as geometry was in Plato's time. In conclusion, I think if Plato lived today, the door to his Academy would read "let no one ignorant of computer science enter."
