#+TITLE: The Scrolling Problem
#+AUTHOR: Tyler J Burns, PhD
#+DATE: July 15, 2022 - July 15, 2022

[[./index.org][Home]]

** Introduction

We make the assumption every day that we have to scroll through our feeds. When we do a Google search, or a PubMed search if you're a biomedical researcher like me, we assume that the relevant results will be at the very top of the list. This might work for specific searches, but not broader ones. In other cases, like when we're checking the news or social media, we face an infinite, constantly updating feed. We find ourselves mindlessly scrolling through a one-dimensional list, with little if any control over what hypernormal, outrage-inducing content will show up next. We do this because there is no alternative.

I define this as the scrolling problem. A lot of data are presented to us as feeds, and in order to "stay updated" we have no choice but to scroll through all of it. This leads us to be reactive if the content has a clickbait element to it (flashy, outrage-inducing). This leads us to be quickly overwhelmed as the feeds force us to rapidly engage in [[https://en.wikipedia.org/wiki/Task_switching_(psychology)][task switching]], which can lead to decreased cognitive performance.

The scrolling problem is a [[https://en.wikipedia.org/wiki/Wicked_problem][wicked problem]]: it is not particularly well-defined, but you know it when you see it. My strategy for solving a wicked problem is to [[./just_paint.org][solve a simpler but related problem]], so I could get some momentum going. In my case, I solved the problem of how to group related texts together in a map. This doesn't remove the need to scroll, but it does allow us to start thinking about alternatives to scrolling.

** From feeds to maps

Here is what viewing your feed as a map would ideally like: rather than having an endless list of tweets, posts, news articles, search results, emails, TODOs, and things of that nature, you have a map. Passages that are similar to each other are near each other based on context. A tweet about dogs and another tweet about dogs would be near each other. A tweet about cats would be near the tweets about dogs because they are related in the context of "pets."

Focusing on social media, imagine your entire twitter feed is a map. You now have more control. Different regions of the map would be about various topics, from celebrity gossip to sports to social justice. You would have control over where in your "feed" you want to go. You could be able to avoid outrage-inducing content if that's what you wish. You could focus on a particular set of topics in your feed so you don't get "sucked in."

Now imagine you're searching PubMed for every scientific paper for a particular disease you're interested in. There could be thousands of papers. Now let's say your results are a map. AI-based papers are to the northeast. Clinical trial papers are to the south. And so on, with each region of the map properly labeled. These broad searches are a good use case for the map view.

** My text-to-map protocol

The tools needed to convert text to maps are open source and readily avilable if you know how to code. My protocol revolves around the use of the BERT language model (there are alternatives, but this one works well enough and is open source). BERT is a pre-trained transformer that takes any text up to 512 words long and converts it into a 768 element vector. Broadly speaking, you can think of this high-dimensional "context space." Data scientists are used to operating with high-dimensional data like this. We know how to program computers to understand this type of data. However, for it to be human readable, we have to somehow turn these 768 dimensional coordinates into a simple XY plane that we're used to.

That's where UMAP comes in. It is a [[https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection][non-linear dimension reduction]] algorithm. It takes each 768 dimensional vector and converts it into a 2 dimensional vector. Vectors (texts) that are similar to each other in the 768 dimensional space will be near each other in 2 dimensions. In other words, you end up with a map, where each point on the map is a text you care about. A text that would otherwise show up as part of your endless scrolling ritual. Now you have a bit more control, as your feed is now a map. UMAP is by no means [[./tjb_dimr_talk.pdf][perfect]]. You can't perfectly compress 768 dimensions without losing information, but you'll be able to see in later sections that it is good enough to be useful in our goal of solving the scrolling problem.

There are other technicalities for the interested user (warning: jargon coming). One can do a preliminary step of determining the effective dimensionality of the data by determining how many principal components explain 95% of the variance. In my experience so far, it's roughly 1/3 of the total dimensions. This could increase both speed and accurracy given you're operating with more signal and less noise.

Additional steps are standard in unsupervised learning and useful here. Clustering the data, so we can access it at the group level is convenient here. Extracting keywords from each cluster helps us determine what clusters are the "sports" versus the "politics that will make me upset" clusters. All of this gets visualized on the map.

The map must be interactive and clickable. I'll get into this later, but I'm using the plotly package to produce such maps. The best practices (in my domain) for clickable "maps" come from flow cytometry analysis, which involve drawing "[[https://docs.flowjo.com/flowjo/graphs-and-gating/gw-gating/][gates]]" around populations of interest. Flow cytometry users will most definitely find the "map" solution to the scrolling problem to be intuitive. 

** A map view of the news

The news is a high-impact low hanging fruit for this type of analysis. I am often bombarded by too much information, as well as hypernormal clickbait. I decided to make a map based on the news article titles to create a "neterws space" that I could query from the map view. The easiest solution to doing this was to use twitter. I collected twitter handles for the major news outlets, from CNN to BBC. I used the twitter API to collect as many tweets from each handle as I possibly could, in order to create a more nuanced "news space."

*** Similar news articles are grouped near each other

We start with an observation that answers the most obvious question: are the news articles being grouped together in a meaninful way? We start by looking at the overall map. The map was clustered by [[https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html][hierarchical dbscan]], an algorithm developed by Leland Mciness to be used along with UMAP. It is not necessarily the optimal clustering algorithm to be used here, but it's a good place to start. Keywords were extracted as follows: for each cluster, all the news tweets were lumped together into one large string, and fed into KeyBERT, which is a BERT-based keyword extractor, consistent with how I created the vector space from each tweet to begin with. Below is what the map looks like. 

[[./images/news_space.png]]

If you zoom in, you can get some interesting nuance. Here, I show a single data point corresponding to a single cluster corresponding to inflation.

[[./images/inflation_cluster.png]]

This doesn't carry much information on its own, so please click on this link that will take you to an interactive html version of this map, where you can spend some time determining if the articles that are similar to each other are grouped near each other.

[[./images/todays_news.html][News on 2022-07-15 Fri 22:00]]

Now we have a bit more control. I update this every day, giving you a map of the news that you can go through rather than coming face to face with media organizations incentivized to get you to click on whatever it is. Now granted, we're looking at tweets here, which are also incentivized in that manner, but on top of the map-level control, we also side-step any provocative images that would also incite one to click. 

** CNN vs Fox News: overlap in "news space"

One use case within news space analysis is to identify topics where conservatives and liberal news agencies are reporting different things. This can help researchers identify news media bias, as well as help ordinary users become more aware of where the narratives differ on a particular topic. Are there regions of news space that are more liberal heavy or more conservative heavy? To start to answer that question, we look at tweets from the main handles of CNN and Fox News. In doing so, we are making the preliminary assumption that CNN and Fox News differences will be liberal versus conservative. In reality, there may simply be differences at the user level. Maybe not all stories are tweeted out. Maybe only a provocative subset. Maybe this provocative subset differs between CNN and Fox News depending on the agency's standards. But like anything new, you have to start somewhere. 

COMING SOON. 





