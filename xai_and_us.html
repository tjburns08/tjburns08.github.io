<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Explainable AI and humanity</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css" />
</head>
<body>
<div id="content">
<h1 class="title">Explainable AI and humanity</h1>
<p>
<a href="./index.html">Home</a>
</p>

<div id="outline-container-org83173a8" class="outline-2">
<h2 id="org83173a8">Explainable AI</h2>
<div class="outline-text-2" id="text-org83173a8">
<p>
Modern AI relies heavily on optimized <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a> that these days do anything from generating <a href="https://www.gwern.net/GPT-3">creative fiction</a> to producing <a href="https://en.wikipedia.org/wiki/DALL-E">images from a verbal description</a>. A common criticism of these types of tools in <a href="https://en.wikipedia.org/wiki/Single-cell_analysis">my field</a> is that they are "black boxes." We can see what they did but we can't see why. We biologists are used to having maximal clarity at every step. Part of our training is is being able to critique prior research, which becomes much harder to do if you just know that some AI tool with some <a href="https://en.wikipedia.org/wiki/List_of_sequence_alignment_software">acronym</a> produced the answer.
</p>

<p>
So this is where the field of <a href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence">explainable artifical intelligence</a> comes in. It's a rather broad pursuit of figuring out what these tools, which are otherwise not human-readable, are actually doing.
</p>

<p>
A lot of the explainable AI tools are are highly visual. For example, a neural net that distinguishes healthy tissue from cancerous tissue in a medical image can be tweaked to show where on a given image the tool is focusing. When you're a physician who has to make life or death diagnostic decisions, you'd much rather know why a "helper AI" made the decision it did.
</p>

<p>
Another example is how I used <a href="./tjb_dimr_talk.pdf">visual tools</a> to give my colleagues intuition around how dimension reduction algorithms perform on their single-cell data. This is what got me interested in explainable AI at large. 
</p>

<p>
Explainable AI, or more broadly speaking, explainable black box algorithms, is a particularly broad field, and each new tool depends heavily on the algorithm itself and the context in which the algorithm is being used. But the concepts are similar, in that you're enabling the user to "get to know" the algorithm in order to be more effective with it and have a more comfortable relationship with it.
</p>
</div>
</div>

<div id="outline-container-orgf83f828" class="outline-2">
<h2 id="orgf83f828">Humanity at large</h2>
<div class="outline-text-2" id="text-orgf83f828">
<p>
And that is what got me thinking in this context about&#x2026;us. Lets make the assumption for a moment that we are, or at least can be represented as, a set of evolutionarily optimized AI tools lumped together in what we call a brain. This would mean that the chunk of tissue we have behind our eyes is really a series of black box algorithms we're stuck with.
</p>

<p>
We spend a huge chunk of our lives getting to know ourselves and others&#x2026;what these black boxes are all about. A lot of this is what give my life <a href="https://www.youtube.com/watch?v=54l8_ewcOlY">meaning</a>. This ranges from determining as a kid whether you'd prefer to eat ice cream or iceberg lettuce on a hot summer day, to determining what you want in a lifetime partner.
</p>

<p>
But there's an abstract level above that. What songs send tingles down your spine? When was the last time you had a spiritual experinece? When you felt like you were part of something bigger? What artwork really stands out to you specifically? Who is your favorite <a href="https://en.wikipedia.org/wiki/Jungian_archetypes">superhero</a> and why?
</p>

<p>
Then there's a "<a href="https://www.youtube.com/watch?v=ERbvKrH-GC4">self expression</a>" layer. If you are given a paintbrush and you <a href="./just_paint.html">just paint</a> something on the fly, what comes out? If you sit and write for an hour about whatever random thought crosses your mind, what comes out? What if you do the same, but it's only allowed to be poetry? 
</p>

<p>
And of course the social layer. What makes you like a person? Dislike a person? What caused that last angry outburst that you had that you didn't see coming?
</p>

<p>
Art, music, moving about the world and experiencing new things, having relationships, all of these are the explainable AI tools of the network of black boxes we call humanity. We have been on this since our <a href="https://en.wikipedia.org/wiki/Timeline_of_human_evolution">emergence</a>. We know what to do here. Accordingly, I'm trying to get intuition around what these black box algorithms are doing by looking at how we get to know ourselves and our bretheren.
</p>

<p>
My best guess right now is that it won't be one simple algorithm that explains all of AI at that same time. So far there are a <a href="https://theaisummer.com/xai/">number of tools and methods</a> to achieve this aim, depending on the context. Perhaps more an dmore these tools will be lumped into interactive dashboards for users. Nonetheless, I think it will be a constant struggle to get to know the machines as we try to get to know ourselves. The good news is if what I am saying carries any weight at all, then we are humans are already equipped to take on the task of explainable AI. We've been working on it for a long time. 
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: June 30 - June 30</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 26.3 (<a href="https://orgmode.org">Org</a> mode 9.1.9)</p>
<p class="validation"></p>
</div>
</body>
</html>
